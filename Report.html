<h1 id="predictionassignment">PredictionAssignment</h1>
<p>Practical Machine Learning - Prediction Assignment Writeup</p>
<h2 id="summary">Summary</h2>
<p>This document is the final report of the Peer Assessment project from the Practical Machine Learning course, which is a part of the Data Science Specialization. It was written and coded in RStudio, using its knitr functions and published in the html format. The purpose of this analysis is to predict the manner in which the six participants performed the exercises described below and to answer the questions of the associated course quiz. The machine learning algorithm, which uses the classe variable in the training set, is applied to the 20 test cases available in the test data. The predictions are submitted to the Course Project Prediction Quiz for grading.</p>
<h2 id="introduction">Introduction</h2>
<p>Devices such as Jawbone Up, Nike FuelBand, and Fitbit can enable collecting a large amount of data about someone’s physical activity. These devices are used by the enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. However, even though these enthusiasts regularly quantify how much of a particular activity they do, they rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of six participants. They were asked to perform barbell lifts correctly and incorrectly in five different ways.</p>
<p>More information is available from the following website: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).</p>
<h2 id="source-of-data">Source of Data</h2>
<p>The data for this project can be found on the following website:</p>
<p>http://groupware.les.inf.puc-rio.br/har.</p>
<p>The training data for this project:</p>
<p>https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</p>
<p>The test data for this project:</p>
<p>https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</p>
<p>The full reference is as follows:</p>
<p>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. “Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13)”. Stuttgart, Germany: ACM SIGCHI, 2013.</p>
<h2 id="data-loading-and-cleaning">Data Loading and Cleaning</h2>
<p>Load the required R packages and set a seed.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lattice)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(caret)
<span class="kw">library</span>(rpart)
<span class="kw">library</span>(rpart.plot)
<span class="kw">library</span>(corrplot)
<span class="kw">library</span>(rattle)
<span class="kw">library</span>(randomForest)
<span class="kw">library</span>(RColorBrewer)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1813</span>)</code></pre>
<p>Load the training and test datasets.</p>
<pre class="sourceCode r"><code class="sourceCode r">url_train &lt;-<span class="st"> &quot;http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;</span>
url_quiz  &lt;-<span class="st"> &quot;http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;</span></code></pre>
<pre class="sourceCode r"><code class="sourceCode r">data_train &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(url_train), <span class="dt">strip.white =</span> <span class="ot">TRUE</span>, <span class="dt">na.strings =</span> <span class="kw">c</span>(<span class="st">&quot;NA&quot;</span>,<span class="st">&quot;&quot;</span>))
data_quiz  &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">url</span>(url_quiz),  <span class="dt">strip.white =</span> <span class="ot">TRUE</span>, <span class="dt">na.strings =</span> <span class="kw">c</span>(<span class="st">&quot;NA&quot;</span>,<span class="st">&quot;&quot;</span>))</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(data_train)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## [1] 19622   160</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(data_quiz)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## [1]  20 160</code></pre>
<p>Create two partitions (75 % and 25 %) within the original training dataset.</p>
<pre class="sourceCode r"><code class="sourceCode r">in_train  &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(data_train$classe, <span class="dt">p=</span><span class="fl">0.75</span>, <span class="dt">list=</span><span class="ot">FALSE</span>)
train_set &lt;-<span class="st"> </span>data_train[ in_train, ]
test_set  &lt;-<span class="st"> </span>data_train[-in_train, ]</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(train_set)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## [1] 14718   160</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(test_set)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## [1] 4904  160</code></pre>
<p>The two datasets (train_set and test_set) have a large number of NA values as well as near-zero-variance (NZV) variables. Both will be removed together with their ID variables.</p>
<pre class="sourceCode r"><code class="sourceCode r">nzv_var &lt;-<span class="st"> </span><span class="kw">nearZeroVar</span>(train_set)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">train_set &lt;-<span class="st"> </span>train_set[ , -nzv_var]
test_set  &lt;-<span class="st"> </span>test_set [ , -nzv_var]</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(train_set)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## [1] 14718   121</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(test_set)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## [1] 4904  121</code></pre>
<p>Remove variables that are mostly NA.</p>
<pre class="sourceCode r"><code class="sourceCode r">na_var &lt;-<span class="st"> </span><span class="kw">sapply</span>(train_set, function(x) <span class="kw">mean</span>(<span class="kw">is.na</span>(x))) &gt;<span class="st"> </span><span class="fl">0.95</span>
train_set &lt;-<span class="st"> </span>train_set[ , na_var ==<span class="st"> </span><span class="ot">FALSE</span>]
test_set  &lt;-<span class="st"> </span>test_set [ , na_var ==<span class="st"> </span><span class="ot">FALSE</span>]</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(train_set)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## [1] 14718    59</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(test_set)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## [1] 4904   59</code></pre>
<p>Since columns 1 to 5 are identification variables only, they will be removed.</p>
<pre class="sourceCode r"><code class="sourceCode r">train_set &lt;-<span class="st"> </span>train_set[ , -(<span class="dv">1</span>:<span class="dv">5</span>)]
test_set  &lt;-<span class="st"> </span>test_set [ , -(<span class="dv">1</span>:<span class="dv">5</span>)]</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(train_set)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## [1] 14718    54</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(test_set)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## [1] 4904   54</code></pre>
<h2 id="correlation-analysis">Correlation Analysis</h2>
<pre class="sourceCode r"><code class="sourceCode r">corr_matrix &lt;-<span class="st"> </span><span class="kw">cor</span>(train_set[ , -<span class="dv">54</span>])
<span class="kw">corrplot</span>(corr_matrix, <span class="dt">order =</span> <span class="st">&quot;FPC&quot;</span>, <span class="dt">method =</span> <span class="st">&quot;circle&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;lower&quot;</span>,
         <span class="dt">tl.cex =</span> <span class="fl">0.6</span>, <span class="dt">tl.col =</span> <span class="kw">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>))</code></pre>
<p><img src="/1.png" /> If two variables are highly correlated their colors are either dark blue (for a positive correlation) or dark red (for a negative corraltions)</p>
<h2 id="prediction-models">Prediction Models</h2>
<h3 id="decision-tree-model">Decision Tree Model</h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1813</span>)
fit_decision_tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(classe ~<span class="st"> </span>., <span class="dt">data =</span> train_set, <span class="dt">method=</span><span class="st">&quot;class&quot;</span>)
<span class="kw">fancyRpartPlot</span>(fit_decision_tree)</code></pre>
<div class="figure">
<img src="/2.png" />
</div>
<p>Predictions of the decision tree model on test_set.</p>
<pre class="sourceCode r"><code class="sourceCode r">predict_decision_tree &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_decision_tree, <span class="dt">newdata =</span> test_set, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)
conf_matrix_decision_tree &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(predict_decision_tree, test_set$classe)
conf_matrix_decision_tree</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1248  173   48   42   39
##          B   49  589   29   92   40
##          C    3   54  667  107   49
##          D   76   89   51  494   98
##          E   19   44   60   69  675
## 
## Overall Statistics
##                                           
##                Accuracy : 0.749           
##                  95% CI : (0.7366, 0.7611)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.6814          
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.8946   0.6207   0.7801   0.6144   0.7492
## Specificity            0.9139   0.9469   0.9474   0.9234   0.9520
## Pos Pred Value         0.8052   0.7372   0.7580   0.6114   0.7785
## Neg Pred Value         0.9562   0.9123   0.9533   0.9243   0.9440
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2545   0.1201   0.1360   0.1007   0.1376
## Detection Prevalence   0.3161   0.1629   0.1794   0.1648   0.1768
## Balanced Accuracy      0.9043   0.7838   0.8638   0.7689   0.8506</code></pre>
<p>The predictive accuracy of the decision tree model is relatively low at 74.9 %.</p>
<h3 id="random-forest-model">Random Forest Model</h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1813</span>)
ctrl_RF &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>, <span class="dt">repeats =</span> <span class="dv">2</span>)
fit_RF  &lt;-<span class="st"> </span><span class="kw">train</span>(classe ~<span class="st"> </span>., <span class="dt">data =</span> train_set, <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
                  <span class="dt">trControl =</span> ctrl_RF, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)
fit_RF$finalModel</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## 
## Call:
##  randomForest(x = x, y = y, mtry = param$mtry, verbose = FALSE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 27
## 
##         OOB estimate of  error rate: 0.22%
## Confusion matrix:
##      A    B    C    D    E  class.error
## A 4184    1    0    0    0 0.0002389486
## B    5 2839    3    1    0 0.0031601124
## C    0    4 2563    0    0 0.0015582392
## D    0    0   10 2401    1 0.0045605307
## E    0    1    0    7 2698 0.0029563932</code></pre>
<p>Predictions of the Random Forest model on test_set.</p>
<pre class="sourceCode r"><code class="sourceCode r">predict_RF &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_RF, <span class="dt">newdata =</span> test_set)
conf_matrix_RF &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(predict_RF, test_set$classe)
conf_matrix_RF</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1394    1    0    0    0
##          B    0  946    2    0    0
##          C    0    2  853    3    0
##          D    0    0    0  801    1
##          E    1    0    0    0  900
## 
## Overall Statistics
##                                          
##                Accuracy : 0.998          
##                  95% CI : (0.9963, 0.999)
##     No Information Rate : 0.2845         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.9974         
##  Mcnemar&#39;s Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9993   0.9968   0.9977   0.9963   0.9989
## Specificity            0.9997   0.9995   0.9988   0.9998   0.9998
## Pos Pred Value         0.9993   0.9979   0.9942   0.9988   0.9989
## Neg Pred Value         0.9997   0.9992   0.9995   0.9993   0.9998
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2843   0.1929   0.1739   0.1633   0.1835
## Detection Prevalence   0.2845   0.1933   0.1750   0.1635   0.1837
## Balanced Accuracy      0.9995   0.9982   0.9982   0.9980   0.9993</code></pre>
<p>The predictive accuracy of the Random Forest model is excellent at 99.8 %.</p>
<h2 id="applying-the-best-predictive-model-to-the-test-data">Applying the Best Predictive Model to the Test Data</h2>
<p>The predictive accuracy of the three models evaluated is as follows:</p>
<p>Decision Tree Model: 74.90 %</p>
<p>Random Forest Model: 99.80 %</p>
<p>The Random Forest model is selected and applied to make predictions on the 20 data points from the original testing dataset (data_quiz).</p>
<pre class="sourceCode r"><code class="sourceCode r">predict_quiz &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_RF, <span class="dt">newdata =</span> data_quiz)
predict_quiz</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E</code></pre>
